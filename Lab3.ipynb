{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1261ed00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic Solution:\n",
      "Coefficients: [1.23636364 1.16969697]\n",
      "Sum of Squared Errors: 5.624242424242426\n",
      "R^2 value: 0.952538038613988\n",
      "\n",
      "Full-batch Gradient Descent:\n",
      "Coefficients: (1.170263693076768, 1.2328099487610318)\n",
      "Sum of Squared Errors: 5.624278989977716\n",
      "R^2 value: 0.9525377300423822\n",
      "\n",
      "Stochastic Gradient Descent:\n",
      "Coefficients: (1.2986755729435908, 0.8967040680508923)\n",
      "Sum of Squared Errors: 7.576246971879953\n",
      "R^2 value: 0.9360654263976376\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n",
    "\n",
    "X = np.vstack([np.ones(len(x)), x]).T\n",
    "beta_hat_normal_eq = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "y_hat_normal_eq = X @ beta_hat_normal_eq\n",
    "SSE_normal_eq = np.sum((y - y_hat_normal_eq)**2)\n",
    "R_squared_normal_eq = 1 - SSE_normal_eq / np.sum((y - np.mean(y))**2)\n",
    "\n",
    "print(\"Analytic Solution:\")\n",
    "print(f\"Coefficients: {beta_hat_normal_eq}\")\n",
    "print(f\"Sum of Squared Errors: {SSE_normal_eq}\")\n",
    "print(f\"R^2 value: {R_squared_normal_eq}\") \n",
    "def full_batch_gradient_descent(x, y, lr=0.01, epochs=1000):\n",
    "    m, b = 0.0, 0.0\n",
    "    N = len(y)\n",
    "    for _ in range(epochs):\n",
    "        y_pred = m*x + b\n",
    "        dm = (-2/N) * np.sum(x * (y - y_pred))\n",
    "        db = (-2/N) * np.sum(y - y_pred)\n",
    "        m -= lr * dm\n",
    "        b -= lr * db\n",
    "    return m, b\n",
    "m_gd, b_gd = full_batch_gradient_descent(x, y)\n",
    "y_pred_gd = m_gd*x + b_gd\n",
    "sse_gd = np.sum((y - y_pred_gd)**2)\n",
    "r_squared_gd = 1 - sse_gd / np.sum((y - np.mean(y))**2)\n",
    "\n",
    "print(\"\\nFull-batch Gradient Descent:\")\n",
    "print(f\"Coefficients: {m_gd, b_gd}\")\n",
    "print(f\"Sum of Squared Errors: {sse_gd}\")\n",
    "print(f\"R^2 value: {r_squared_gd}\")\n",
    "def stochastic_gradient_descent(x, y, lr=0.01, epochs=1000):\n",
    "    m, b = 0.0, 0.0\n",
    "    N = len(y)\n",
    "    for _ in range(epochs):\n",
    "        for i in range(N):\n",
    "            y_pred = m*x[i] + b\n",
    "            dm = -2 * x[i] * (y[i] - y_pred)\n",
    "            db = -2 * (y[i] - y_pred)\n",
    "            m -= lr * dm\n",
    "            b -= lr * db\n",
    "    return m, b\n",
    "m_sgd, b_sgd = stochastic_gradient_descent(x, y)\n",
    "y_pred_sgd = m_sgd*x + b_sgd\n",
    "sse_sgd = np.sum((y - y_pred_sgd)**2)\n",
    "r_squared_sgd = 1 - sse_sgd / np.sum((y - np.mean(y))**2)\n",
    "\n",
    "print(\"\\nStochastic Gradient Descent:\")\n",
    "print(f\"Coefficients: {m_sgd, b_sgd}\")\n",
    "print(f\"Sum of Squared Errors: {sse_sgd}\")\n",
    "print(f\"R^2 value: {r_squared_sgd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32551081",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGradientDescent():\n",
    "    def __init__(self, lr=0.0001, max_iter=10000, tolerance=1e-6):\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.weights = np.zeros(X_with_bias.shape[1])\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            gradients = -2 * X_with_bias.T.dot(y - X_with_bias.dot(self.weights))\n",
    "            if np.all(np.abs(gradients) < self.tolerance):\n",
    "                break\n",
    "            self.weights -= self.lr * gradients\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_with_bias.dot(self.weights)\n",
    "\n",
    "class LinearRegressionStochasticGradientDescent():\n",
    "    def __init__(self, lr=0.0001, max_iter=1000, tolerance=1e-6, batch_size=1):\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.tolerance = tolerance\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.weights = np.zeros(X_with_bias.shape[1])\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            indices = np.random.permutation(X_with_bias.shape[0])\n",
    "            for start_idx in range(0, X_with_bias.shape[0], self.batch_size):\n",
    "                batch_indices = indices[start_idx:start_idx + self.batch_size]\n",
    "                X_batch = X_with_bias[batch_indices]\n",
    "                y_batch = y[batch_indices]\n",
    "                gradients = -2 * X_batch.T.dot(y_batch - X_batch.dot(self.weights))\n",
    "                if np.all(np.abs(gradients) < self.tolerance):\n",
    "                    break\n",
    "                self.weights -= self.lr * gradients\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_with_bias.dot(self.weights)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
